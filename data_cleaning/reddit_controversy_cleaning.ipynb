{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45cab438-61ed-4359-91d0-46ab8135fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c29a5d4-6be1-46bd-a0c8-e5b2511f6500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/DAVIDSON/ridoctor/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reading in the data and visualizing it in an accessible manner\n",
    "df = pd.read_json('../json_files/merged_file.json')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "pd.set_option(\"display.max_colwidth\", 80)  # or None to see full text\n",
    "\n",
    "# Cleaning data\n",
    "LABEL_COL = \"id\" \n",
    "TEXT_COL = \"text\"\n",
    "df = df[df[LABEL_COL].notna() & df[TEXT_COL].notna()]\n",
    "\n",
    "df.head()\n",
    "\n",
    "def preprocess_text(t: str) -> str:\n",
    "    t = t.lower()\n",
    "    \n",
    "    # remove URLs\n",
    "    t = re.sub(r\"http\\S+|www\\S+\", \"\", t)\n",
    "    \n",
    "    # remove user mentions (Reddit / Twitter style)\n",
    "    t = re.sub(r\"u\\/\\w+|@\\w+\", \"\", t)\n",
    "    \n",
    "    # remove extra whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    \n",
    "    return t\n",
    "\n",
    "TEXT_COL = \"text\"\n",
    "df[\"text_clean\"] = df[TEXT_COL].apply(preprocess_text)\n",
    "\n",
    "REPLY_COL = \"comment_count\"\n",
    "UPVOTE_COL = \"vote_total\"\n",
    "df[\"comment_upvote_ratio\"] = df[REPLY_COL] / (df[UPVOTE_COL] + 1)\n",
    "\n",
    "\n",
    "# Adding sentiment analysis\n",
    "df[\"sentiment\"] = df[\"text\"].apply(lambda x:TextBlob(x).sentiment.polarity)\n",
    "nltk.download(\"vader_lexicon\")\n",
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fe428a7-2ba3-4e4a-9890-7e84912c707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_features(text):\n",
    "    scores = vader.polarity_scores(text)\n",
    "    return pd.Series({\n",
    "        \"vader_neg\": scores[\"neg\"],\n",
    "        \"vader_neu\": scores[\"neu\"],\n",
    "        \"vader_pos\": scores[\"pos\"],\n",
    "        \"vader_compound\": scores[\"compound\"],\n",
    "    })\n",
    "\n",
    "vader_df = df[\"text_clean\"].apply(vader_features)\n",
    "df = pd.concat([df, vader_df], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2e4e53d-397d-4e16-a5a7-78425568a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding conflict word count\n",
    "CONFLICT_WORDS = [\n",
    "    \"but\", \"however\", \"actually\", \"wrong\", \"disagree\",\n",
    "    \"no\", \"not\", \"never\", \"false\"\n",
    "]\n",
    "\n",
    "def conflict_features(text):\n",
    "    tokens = text.split()\n",
    "    return pd.Series({\n",
    "        \"conflict_count\": sum(t in CONFLICT_WORDS for t in tokens),\n",
    "        \"has_conflict\": int(any(t in CONFLICT_WORDS for t in tokens)),\n",
    "        \"exclamations\": text.count(\"!\"),\n",
    "        \"questions\": text.count(\"?\"),\n",
    "        \"all_caps_ratio\": sum(w.isupper() for w in tokens) / (len(tokens) + 1)\n",
    "    })\n",
    "\n",
    "conflict_df = df[\"text_clean\"].apply(conflict_features)\n",
    "df = pd.concat([df, conflict_df], axis=1)\n",
    "\n",
    "\n",
    "df[\"abs_vader_compound\"] = df[\"vader_compound\"].abs()\n",
    "df[\"abs_sentiment\"] = df[\"sentiment\"].abs()  # TextBlob polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a155a713-083d-48b4-93d9-f031fa6b3410",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISAGREE_WORDS = [\n",
    "    \"disagree\", \"wrong\", \"false\", \"misleading\",\n",
    "    \"no\", \"not\", \"never\", \"nonsense\", \"ridiculous\"\n",
    "]\n",
    "\n",
    "def disagreement_features(text):\n",
    "    tokens = text.split()\n",
    "    count = sum(w in tokens for w in DISAGREE_WORDS)\n",
    "    return pd.Series({\n",
    "        \"disagree_count\": count,\n",
    "        \"has_disagree\": int(count > 0)\n",
    "    })\n",
    "\n",
    "disagree_df = df[\"text_clean\"].apply(disagreement_features)\n",
    "df = pd.concat([df, disagree_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2c0e67a-70fc-4948-9772-ed40e8e226f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hour\"] = df[\"created_at\"].dt.hour\n",
    "df[\"day_of_week\"] = df[\"created_at\"].dt.dayofweek\n",
    "df[\"is_weekend\"] = df[\"day_of_week\"].isin([5, 6]).astype(int)\n",
    "\n",
    "df[\"post_length\"] = df[\"text_clean\"].str.split().str.len()\n",
    "\n",
    "df[\"first_person_count\"] = df[\"text_clean\"].str.count(\n",
    "    r\"\\b(i|me|my|mine|we|us|our|ours)\\b\"\n",
    ")\n",
    "\n",
    "df[\"second_person_count\"] = df[\"text_clean\"].str.count(\n",
    "    r\"\\b(you|your|yours|u)\\b\"\n",
    ")\n",
    "\n",
    "df[\"first_person_ratio\"] = df[\"first_person_count\"] / (df[\"post_length\"] + 1)\n",
    "df[\"second_person_ratio\"] = df[\"second_person_count\"] / (df[\"post_length\"] + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54e019fa-3542-4d27-98b9-2b7f26306142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in as CSV file\n",
    "df.to_csv(\"../csv_files/reddit_controversal_df_features.csv\", index=False)\n",
    "print(\"CSV created, locate in csv_files folder\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (CSC371)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
