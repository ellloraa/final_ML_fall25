{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "459d4032-a8c5-42ab-93d9-441f154022cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8cac917-ae3c-45fa-8e2b-4d33db5e0848",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "with open(\"../json_files/more_yikyak_posts.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, start=1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # skip blank lines\n",
    "        try:\n",
    "            data.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Skipping bad JSON on line {i}: {e}\")\n",
    "\n",
    "\n",
    "# Cleaning data\n",
    "df = pd.read_json(\"../json_files/more_yikyak_posts.jsonl\", lines=True)\n",
    "LABEL_COL = \"id\" \n",
    "TEXT_COL = \"text\"\n",
    "df = df[df[LABEL_COL].notna() & df[TEXT_COL].notna()]\n",
    "\n",
    "\n",
    "def preprocess_text(t: str) -> str:\n",
    "\n",
    "    t = emoji.demojize(t, delimiters=(\" \", \" \")) #convert emojis to text\n",
    "    t = t.lower()\n",
    "    \n",
    "    # remove URLs\n",
    "    t = re.sub(r\"http\\S+|www\\S+\", \"\", t)\n",
    "    \n",
    "    # remove user mentions (Reddit / Twitter style)\n",
    "    t = re.sub(r\"u\\/\\w+|@\\w+\", \"\", t)\n",
    "    \n",
    "    # remove extra whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    \n",
    "    return t\n",
    "\n",
    "\n",
    "df[\"text_clean\"] = df[TEXT_COL].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40db2424-9a3d-4494-9970-8dc5b76dec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_length\"] = df[\"text\"].apply(len)\n",
    "df[\"comment_ratio\"] = df[\"comment_count\"] / (df[\"vote_total\"] + 1)\n",
    "threshold = df[\"vote_total\"].quantile(0.90)\n",
    "df[\"high_engagement\"] = (df[\"vote_total\"] >= threshold).astype(int)\n",
    "df[\"created_at\"] = pd.to_datetime(df[\"created_at\"])\n",
    "df[\"created_hour\"] = df[\"created_at\"].dt.hour\n",
    "df[\"created_day\"] = df[\"created_at\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0c383a0-f168-4dbd-bd63-14ff9f85eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"first_person_count\"] = df[\"text_clean\"].str.count(\n",
    "    r\"\\b(i|me|my|mine|we|us|our|ours)\\b\"\n",
    ")\n",
    "\n",
    "df[\"second_person_count\"] = df[\"text_clean\"].str.count(\n",
    "    r\"\\b(you|your|yours|u)\\b\"\n",
    ")\n",
    "\n",
    "df[\"first_person_ratio\"] = df[\"first_person_count\"] / (df[\"text_length\"] + 1)\n",
    "df[\"second_person_ratio\"] = df[\"second_person_count\"] / (df[\"text_length\"] + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27a9bc29-c620-4e46-8e03-2f545e1ff1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISAGREE_WORDS = [\n",
    "    \"disagree\", \"wrong\", \"false\", \"misleading\",\n",
    "    \"no\", \"not\", \"never\", \"nonsense\", \"ridiculous\"\n",
    "]\n",
    "\n",
    "def disagreement_features(text):\n",
    "    tokens = text.split()\n",
    "    count = sum(w in tokens for w in DISAGREE_WORDS)\n",
    "    return pd.Series({\n",
    "        \"disagree_count\": count,\n",
    "        \"has_disagree\": int(count > 0)\n",
    "    })\n",
    "\n",
    "disagree_df = df[\"text_clean\"].apply(disagreement_features)\n",
    "df = pd.concat([df, disagree_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5cc4cb2-0586-4b0f-9a16-f19c49654e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding conflict word count\n",
    "CONFLICT_WORDS = [\n",
    "    \"but\", \"however\", \"actually\", \"wrong\", \"disagree\",\n",
    "    \"no\", \"not\", \"never\", \"false\"\n",
    "]\n",
    "\n",
    "def conflict_features(text):\n",
    "    tokens = text.split()\n",
    "    return pd.Series({\n",
    "        \"conflict_count\": sum(t in CONFLICT_WORDS for t in tokens),\n",
    "        \"has_conflict\": int(any(t in CONFLICT_WORDS for t in tokens)),\n",
    "        \"exclamations\": text.count(\"!\"),\n",
    "        \"questions\": text.count(\"?\"),\n",
    "        \"all_caps_ratio\": sum(w.isupper() for w in tokens) / (len(tokens) + 1)\n",
    "    })\n",
    "\n",
    "conflict_df = df[\"text_clean\"].apply(conflict_features)\n",
    "df = pd.concat([df, conflict_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffc05abc-fe62-447e-8da0-d24ec82fbf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/DAVIDSON/ridoctor/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"vader_lexicon\")\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_features(text):\n",
    "    scores = vader.polarity_scores(text)\n",
    "    return pd.Series({\n",
    "        \"vader_neg\": scores[\"neg\"],\n",
    "        \"vader_neu\": scores[\"neu\"],\n",
    "        \"vader_pos\": scores[\"pos\"],\n",
    "        \"vader_compound\": scores[\"compound\"],\n",
    "    })\n",
    "\n",
    "vader_df = df[\"text_clean\"].apply(vader_features)\n",
    "df = pd.concat([df, vader_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "645e36ac-85ab-4432-82d6-8044e3361a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping no text posts\n",
    "df = df[df[\"text\"].notna() & (df[\"text\"].str.strip() != \"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eaac6e7-cded-4ba8-9552-bec8bf1c6327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check duplicates in raw text\n",
    "df[\"text\"].duplicated().any()\n",
    "\n",
    "# Or check duplicates in cleaned text\n",
    "df[\"text_clean\"].duplicated().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e925c13-53aa-4da7-a801-b46a996193fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(106)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text_clean\"].duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aea7ee69-6460-4a66-9860-abe24f93c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dupes = df.drop_duplicates(subset=[\"text_clean\"], keep=\"first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e13cbb-6350-468c-ad54-f04f58df0b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_no_dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c54b8ff-1ed5-4097-a37b-62f2958870bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Separate classes\n",
    "df_0 = df[df[\"high_engagement\"] == 0]\n",
    "df_1 = df[df[\"high_engagement\"] == 1]\n",
    "\n",
    "# Undersample class 0 to match class 1 size\n",
    "df_0_under = df_0.sample(n=len(df_1), random_state=42)\n",
    "\n",
    "# Combine and shuffle\n",
    "df_under = pd.concat([df_0_under, df_1]).sample(frac=1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f3c162b-254a-40f2-9cf9-212e1890e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Burstiness features (past 2 hours) ---\n",
    "# Put this AFTER: df[\"created_at\"] = pd.to_datetime(df[\"created_at\"])\n",
    "\n",
    "# Ensure timezone handling is consistent\n",
    "df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], utc=True, errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"created_at\"])\n",
    "\n",
    "# Sort so rolling/shift only uses the past (prevents leakage)\n",
    "df = df.sort_values(\"created_at\").reset_index(drop=True)\n",
    "\n",
    "# Use a time index for rolling windows\n",
    "df = df.set_index(\"created_at\")\n",
    "\n",
    "WINDOW = \"2h\"\n",
    "\n",
    "# 1) Global activity in prior 2 hours\n",
    "# rolling count includes the current row, so subtract 1 to get \"previous\"\n",
    "df[\"posts_prev_2h_all\"] = df[\"id\"].rolling(WINDOW).count() - 1\n",
    "df[\"posts_prev_2h_all\"] = df[\"posts_prev_2h_all\"].clip(lower=0).fillna(0)\n",
    "\n",
    "# 2) Group activity in prior 2 hours\n",
    "# rolling per group, again subtract 1 to exclude current post\n",
    "df[\"posts_prev_2h_group\"] = (\n",
    "    df.groupby(\"group_id\")[\"id\"]\n",
    "      .rolling(WINDOW)\n",
    "      .count()\n",
    "      .reset_index(level=0, drop=True)\n",
    "      - 1\n",
    ")\n",
    "df[\"posts_prev_2h_group\"] = df[\"posts_prev_2h_group\"].clip(lower=0).fillna(0)\n",
    "\n",
    "# 3) Relative burstiness: \"is the group unusually busy compared to the whole app?\"\n",
    "df[\"rel_posts_prev_2h\"] = df[\"posts_prev_2h_group\"] / (df[\"posts_prev_2h_all\"] + 1)\n",
    "\n",
    "# 4) Simple \"burst flag\" within each group\n",
    "# Compare current 2h activity to group's typical 2h activity (rolling mean/std over last 30 days)\n",
    "# If you don't have a full year in every group, this still behaves fine due to min_periods.\n",
    "BASELINE = \"30d\"\n",
    "grp_roll_mean = (\n",
    "    df.groupby(\"group_id\")[\"posts_prev_2h_group\"]\n",
    "      .rolling(BASELINE, min_periods=50)\n",
    "      .mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "grp_roll_std = (\n",
    "    df.groupby(\"group_id\")[\"posts_prev_2h_group\"]\n",
    "      .rolling(BASELINE, min_periods=50)\n",
    "      .std()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "df[\"burst_z_group\"] = (df[\"posts_prev_2h_group\"] - grp_roll_mean) / (grp_roll_std + 1e-6)\n",
    "df[\"burst_z_group\"] = df[\"burst_z_group\"].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "# Flag: 1 means \"unusually busy right now\"\n",
    "df[\"burst_flag_group\"] = (df[\"burst_z_group\"] >= 1.0).astype(int)\n",
    "\n",
    "# Return to normal index for saving\n",
    "df = df.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "affa7033-6d4b-4290-a7f0-3b6007e8dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../yikyak_metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87ea06cb-f811-473b-b37d-d3b7756c688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EDA PLOTS (optional) ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "def make_eda_plots(df: pd.DataFrame, out_dir: str = \"plots\", sample_n: int = 3000):\n",
    "    out = Path(out_dir)\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Saving plots to:\", out.resolve())\n",
    "\n",
    "\n",
    "    # 1) Correlation heatmap (numeric columns only)\n",
    "    num = df.select_dtypes(include=\"number\")\n",
    "    if num.shape[1] >= 2:\n",
    "        corr = num.corr(numeric_only=True)\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"viridis\")\n",
    "        plt.title(\"Correlation heatmap (numeric features)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out / \"corr_heatmap.png\", dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    # 2) Pairplot \n",
    "    cols = [c for c in [\"vote_total\", \"comment_count\", \"text_length\", \"comment_ratio\"] if c in df.columns]\n",
    "    if len(cols) >= 2:\n",
    "        df_s = df[cols].dropna()\n",
    "        if len(df_s) > sample_n:\n",
    "            df_s = df_s.sample(sample_n, random_state=42)\n",
    "\n",
    "        g = sns.pairplot(df_s, diag_kind=\"hist\")\n",
    "        g.fig.suptitle(\"Pairplot (sampled)\", y=1.02)\n",
    "        g.savefig(out / \"pairplot.png\", dpi=200)\n",
    "        plt.close(\"all\")\n",
    "\n",
    "    # 3) Bar plots like your “mean Answers/Views by Tag”\n",
    "    # Example: mean vote_total and mean comment_count by created_hour\n",
    "    if \"created_hour\" in df.columns:\n",
    "        means = df.groupby(\"created_hour\")[[\"vote_total\", \"comment_count\"]].mean(numeric_only=True)\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        means[\"comment_count\"].plot(kind=\"bar\")\n",
    "        plt.ylabel(\"Mean comment_count\")\n",
    "        plt.xlabel(\"created_hour\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out / \"mean_comment_count_by_hour.png\", dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        means[\"vote_total\"].plot(kind=\"bar\")\n",
    "        plt.ylabel(\"Mean vote_total\")\n",
    "        plt.xlabel(\"created_hour\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out / \"mean_vote_total_by_hour.png\", dpi=200)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa7ccb04-07c6-490a-a448-ed2f0d733c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving plots to: /home/DAVIDSON/ridoctor/Workspace/CSC371/final_ML_fall25/data_cleaning/plots\n"
     ]
    }
   ],
   "source": [
    " make_eda_plots(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9954b3d-b3eb-42ae-b4ce-8ef98fca1b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pub/envs/CSC371-shared/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/pub/envs/CSC371-shared/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "for c in [\"vote_total\", \"comment_count\", \"posts_prev_2h_all\"]:\n",
    "    df[f\"log_{c}\"] = np.log1p(df[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ae09ee9-72c8-42d7-9038-2af143ad0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"comment_ratio\"] = df[\"comment_count\"] / (df[\"vote_total\"] + 1)\n",
    "df[\"comment_ratio\"] = df[\"comment_ratio\"].clip(0, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ed62502-691e-4d69-87f6-c40f9d6d39ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [\"vote_total\", \"comment_count\", \"posts_prev_2h_all\"]:\n",
    "    safe = df[c].clip(lower=0)        # remove negatives\n",
    "    df[f\"log_{c}\"] = np.log1p(safe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bb0ddf2-18cf-46d7-ad6b-fc88a022b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../yikyak_metadata.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (CSC371)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
