{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "459d4032-a8c5-42ab-93d9-441f154022cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d8cac917-ae3c-45fa-8e2b-4d33db5e0848",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "with open(\"../more_yikyak_posts.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, start=1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # skip blank lines\n",
    "        try:\n",
    "            data.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Skipping bad JSON on line {i}: {e}\")\n",
    "\n",
    "\n",
    "# Cleaning data\n",
    "#df.head()\n",
    "\n",
    "df = pd.read_json(\"../more_yikyak_posts.jsonl\", lines=True)\n",
    "LABEL_COL = \"id\" \n",
    "TEXT_COL = \"text\"\n",
    "df = df[df[LABEL_COL].notna() & df[TEXT_COL].notna()]\n",
    "\n",
    "\n",
    "def preprocess_text(t: str) -> str:\n",
    "\n",
    "    t = emoji.demojize(t, delimiters=(\" \", \" \")) #convert emojis to text\n",
    "    t = t.lower()\n",
    "    \n",
    "    # remove URLs\n",
    "    t = re.sub(r\"http\\S+|www\\S+\", \"\", t)\n",
    "    \n",
    "    # remove user mentions (Reddit / Twitter style)\n",
    "    t = re.sub(r\"u\\/\\w+|@\\w+\", \"\", t)\n",
    "    \n",
    "    # remove extra whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    \n",
    "    return t\n",
    "\n",
    "\n",
    "df[\"text_clean\"] = df[TEXT_COL].apply(preprocess_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "40db2424-9a3d-4494-9970-8dc5b76dec44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>vote_total</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>alias</th>\n",
       "      <th>group_id</th>\n",
       "      <th>index_code</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_length</th>\n",
       "      <th>comment_ratio</th>\n",
       "      <th>high_engagement</th>\n",
       "      <th>created_hour</th>\n",
       "      <th>created_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d50e6e42-323e-404b-a349-bec42e614b19</td>\n",
       "      <td>Jarvis, Iâ€™m running low on Yakarma</td>\n",
       "      <td>2025-12-14 06:32:26.933000+00:00</td>\n",
       "      <td>-3</td>\n",
       "      <td>3</td>\n",
       "      <td>Anonymous</td>\n",
       "      <td>1fcad7b1-fce2-4ae1-bd48-bd1917b62d98</td>\n",
       "      <td>Fpp9kuO3</td>\n",
       "      <td>jarvis, iâ€™m running low on yakarma</td>\n",
       "      <td>34</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2025-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c2a438d4-eac2-4a2b-975b-bdf3930c809b</td>\n",
       "      <td>Pray for Brown ðŸ¤Ž</td>\n",
       "      <td>2025-12-14 05:08:52.476000+00:00</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>Anonymous</td>\n",
       "      <td>1fcad7b1-fce2-4ae1-bd48-bd1917b62d98</td>\n",
       "      <td>2JDyMyq5</td>\n",
       "      <td>pray for brown brown_heart</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4e015ca3-970e-420e-bf09-f5c115791696</td>\n",
       "      <td>The bits are on a generational run right now b...</td>\n",
       "      <td>2025-12-14 05:02:41.856000+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Anonymous</td>\n",
       "      <td>1fcad7b1-fce2-4ae1-bd48-bd1917b62d98</td>\n",
       "      <td>R3h2lBvx</td>\n",
       "      <td>the bits are on a generational run right now b...</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91daece4-43c6-4ea0-94b3-b5d2d0c327ae</td>\n",
       "      <td>Thank god Iâ€™m moving out rn ðŸ˜°</td>\n",
       "      <td>2025-12-14 04:33:28.936000+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Anonymous</td>\n",
       "      <td>1fcad7b1-fce2-4ae1-bd48-bd1917b62d98</td>\n",
       "      <td>sJ3N2nZO</td>\n",
       "      <td>thank god iâ€™m moving out rn anxious_face_with_...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9b2de0fe-11d2-465f-80b5-4a996d8d3c5f</td>\n",
       "      <td>BRING BACK TEA APP. BRING BACK TEA APP. BRING ...</td>\n",
       "      <td>2025-12-14 04:02:22.066000+00:00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>Anonymous</td>\n",
       "      <td>1fcad7b1-fce2-4ae1-bd48-bd1917b62d98</td>\n",
       "      <td>7Jx4kSo0</td>\n",
       "      <td>bring back tea app. bring back tea app. bring ...</td>\n",
       "      <td>86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-12-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  d50e6e42-323e-404b-a349-bec42e614b19   \n",
       "1  c2a438d4-eac2-4a2b-975b-bdf3930c809b   \n",
       "2  4e015ca3-970e-420e-bf09-f5c115791696   \n",
       "3  91daece4-43c6-4ea0-94b3-b5d2d0c327ae   \n",
       "4  9b2de0fe-11d2-465f-80b5-4a996d8d3c5f   \n",
       "\n",
       "                                                text  \\\n",
       "0                 Jarvis, Iâ€™m running low on Yakarma   \n",
       "1                                   Pray for Brown ðŸ¤Ž   \n",
       "2  The bits are on a generational run right now b...   \n",
       "3                      Thank god Iâ€™m moving out rn ðŸ˜°   \n",
       "4  BRING BACK TEA APP. BRING BACK TEA APP. BRING ...   \n",
       "\n",
       "                        created_at  vote_total  comment_count      alias  \\\n",
       "0 2025-12-14 06:32:26.933000+00:00          -3              3  Anonymous   \n",
       "1 2025-12-14 05:08:52.476000+00:00          21              0  Anonymous   \n",
       "2 2025-12-14 05:02:41.856000+00:00           1              0  Anonymous   \n",
       "3 2025-12-14 04:33:28.936000+00:00           3              0  Anonymous   \n",
       "4 2025-12-14 04:02:22.066000+00:00          20              0  Anonymous   \n",
       "\n",
       "                               group_id index_code  \\\n",
       "0  1fcad7b1-fce2-4ae1-bd48-bd1917b62d98   Fpp9kuO3   \n",
       "1  1fcad7b1-fce2-4ae1-bd48-bd1917b62d98   2JDyMyq5   \n",
       "2  1fcad7b1-fce2-4ae1-bd48-bd1917b62d98   R3h2lBvx   \n",
       "3  1fcad7b1-fce2-4ae1-bd48-bd1917b62d98   sJ3N2nZO   \n",
       "4  1fcad7b1-fce2-4ae1-bd48-bd1917b62d98   7Jx4kSo0   \n",
       "\n",
       "                                          text_clean  text_length  \\\n",
       "0                 jarvis, iâ€™m running low on yakarma           34   \n",
       "1                         pray for brown brown_heart           16   \n",
       "2  the bits are on a generational run right now b...          100   \n",
       "3  thank god iâ€™m moving out rn anxious_face_with_...           29   \n",
       "4  bring back tea app. bring back tea app. bring ...           86   \n",
       "\n",
       "   comment_ratio  high_engagement  created_hour created_day  \n",
       "0           -1.5                0             6  2025-12-14  \n",
       "1            0.0                0             5  2025-12-14  \n",
       "2            0.0                0             5  2025-12-14  \n",
       "3            0.0                0             4  2025-12-14  \n",
       "4            0.0                0             4  2025-12-14  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text_length\"] = df[\"text\"].apply(len)\n",
    "df[\"comment_ratio\"] = df[\"comment_count\"] / (df[\"vote_total\"] + 1)\n",
    "threshold = df[\"vote_total\"].quantile(0.90)\n",
    "df[\"high_engagement\"] = (df[\"vote_total\"] >= threshold).astype(int)\n",
    "df[\"created_at\"] = pd.to_datetime(df[\"created_at\"])\n",
    "df[\"created_hour\"] = df[\"created_at\"].dt.hour\n",
    "df[\"created_day\"] = df[\"created_at\"].dt.date\n",
    "df.to_csv(\"../csv_files/yikyak_engagement.csv\", index=False)\n",
    "\n",
    "df.head()\n",
    "#df[\"hour\"] = df[\"created_at\"].dt.hour\n",
    "#df[\"day_of_week\"] = df[\"created_at\"].dt.dayofweek\n",
    "#df[\"is_weekend\"] = df[\"day_of_week\"].isin([5, 6]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b0c383a0-f168-4dbd-bd63-14ff9f85eae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19558, 18)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df[\"first_person_count\"] = df[\"text_clean\"].str.count(\n",
    "    r\"\\b(i|me|my|mine|we|us|our|ours)\\b\"\n",
    ")\n",
    "\n",
    "df[\"second_person_count\"] = df[\"text_clean\"].str.count(\n",
    "    r\"\\b(you|your|yours|u)\\b\"\n",
    ")\n",
    "\n",
    "df[\"first_person_ratio\"] = df[\"first_person_count\"] / (df[\"text_length\"] + 1)\n",
    "df[\"second_person_ratio\"] = df[\"second_person_count\"] / (df[\"text_length\"] + 1)\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "27a9bc29-c620-4e46-8e03-2f545e1ff1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISAGREE_WORDS = [\n",
    "    \"disagree\", \"wrong\", \"false\", \"misleading\",\n",
    "    \"no\", \"not\", \"never\", \"nonsense\", \"ridiculous\"\n",
    "]\n",
    "\n",
    "def disagreement_features(text):\n",
    "    tokens = text.split()\n",
    "    count = sum(w in tokens for w in DISAGREE_WORDS)\n",
    "    return pd.Series({\n",
    "        \"disagree_count\": count,\n",
    "        \"has_disagree\": int(count > 0)\n",
    "    })\n",
    "\n",
    "disagree_df = df[\"text_clean\"].apply(disagreement_features)\n",
    "df = pd.concat([df, disagree_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d5cc4cb2-0586-4b0f-9a16-f19c49654e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding conflict word count\n",
    "CONFLICT_WORDS = [\n",
    "    \"but\", \"however\", \"actually\", \"wrong\", \"disagree\",\n",
    "    \"no\", \"not\", \"never\", \"false\"\n",
    "]\n",
    "\n",
    "def conflict_features(text):\n",
    "    tokens = text.split()\n",
    "    return pd.Series({\n",
    "        \"conflict_count\": sum(t in CONFLICT_WORDS for t in tokens),\n",
    "        \"has_conflict\": int(any(t in CONFLICT_WORDS for t in tokens)),\n",
    "        \"exclamations\": text.count(\"!\"),\n",
    "        \"questions\": text.count(\"?\"),\n",
    "        \"all_caps_ratio\": sum(w.isupper() for w in tokens) / (len(tokens) + 1)\n",
    "    })\n",
    "\n",
    "conflict_df = df[\"text_clean\"].apply(conflict_features)\n",
    "df = pd.concat([df, conflict_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ffc05abc-fe62-447e-8da0-d24ec82fbf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/DAVIDSON/eldevulapally/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"vader_lexicon\")\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_features(text):\n",
    "    scores = vader.polarity_scores(text)\n",
    "    return pd.Series({\n",
    "        \"vader_neg\": scores[\"neg\"],\n",
    "        \"vader_neu\": scores[\"neu\"],\n",
    "        \"vader_pos\": scores[\"pos\"],\n",
    "        \"vader_compound\": scores[\"compound\"],\n",
    "    })\n",
    "\n",
    "vader_df = df[\"text_clean\"].apply(vader_features)\n",
    "df = pd.concat([df, vader_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "73f57c56-bd24-4dd7-a558-342c13dfc604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19558, 29)\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "645e36ac-85ab-4432-82d6-8044e3361a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19232, 29)\n"
     ]
    }
   ],
   "source": [
    "# Dropping no text posts\n",
    "df = df[df[\"text\"].notna() & (df[\"text\"].str.strip() != \"\")]\n",
    "df.head()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1eaac6e7-cded-4ba8-9552-bec8bf1c6327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check duplicates in raw text\n",
    "df[\"text\"].duplicated().any()\n",
    "\n",
    "# Or check duplicates in cleaned text\n",
    "df[\"text_clean\"].duplicated().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4e925c13-53aa-4da7-a801-b46a996193fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(106)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text_clean\"].duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "aea7ee69-6460-4a66-9860-abe24f93c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dupes = df.drop_duplicates(subset=[\"text_clean\"], keep=\"first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b8e13cbb-6350-468c-ad54-f04f58df0b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_no_dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2c54b8ff-1ed5-4097-a37b-62f2958870bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Separate classes\n",
    "df_0 = df[df[\"high_engagement\"] == 0]\n",
    "df_1 = df[df[\"high_engagement\"] == 1]\n",
    "\n",
    "# Undersample class 0 to match class 1 size\n",
    "df_0_under = df_0.sample(n=len(df_1), random_state=42)\n",
    "\n",
    "# Combine and shuffle\n",
    "df_under = pd.concat([df_0_under, df_1]).sample(frac=1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2f3c162b-254a-40f2-9cf9-212e1890e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Burstiness features (past 2 hours) ---\n",
    "# Put this AFTER: df[\"created_at\"] = pd.to_datetime(df[\"created_at\"])\n",
    "\n",
    "# Ensure timezone handling is consistent\n",
    "df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], utc=True, errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"created_at\"])\n",
    "\n",
    "# Sort so rolling/shift only uses the past (prevents leakage)\n",
    "df = df.sort_values(\"created_at\").reset_index(drop=True)\n",
    "\n",
    "# Use a time index for rolling windows\n",
    "df = df.set_index(\"created_at\")\n",
    "\n",
    "WINDOW = \"2h\"\n",
    "\n",
    "# 1) Global activity in prior 2 hours\n",
    "# rolling count includes the current row, so subtract 1 to get \"previous\"\n",
    "df[\"posts_prev_2h_all\"] = df[\"id\"].rolling(WINDOW).count() - 1\n",
    "df[\"posts_prev_2h_all\"] = df[\"posts_prev_2h_all\"].clip(lower=0).fillna(0)\n",
    "\n",
    "# 2) Group activity in prior 2 hours\n",
    "# rolling per group, again subtract 1 to exclude current post\n",
    "df[\"posts_prev_2h_group\"] = (\n",
    "    df.groupby(\"group_id\")[\"id\"]\n",
    "      .rolling(WINDOW)\n",
    "      .count()\n",
    "      .reset_index(level=0, drop=True)\n",
    "      - 1\n",
    ")\n",
    "df[\"posts_prev_2h_group\"] = df[\"posts_prev_2h_group\"].clip(lower=0).fillna(0)\n",
    "\n",
    "# 3) Relative burstiness: \"is the group unusually busy compared to the whole app?\"\n",
    "df[\"rel_posts_prev_2h\"] = df[\"posts_prev_2h_group\"] / (df[\"posts_prev_2h_all\"] + 1)\n",
    "\n",
    "# 4) Simple \"burst flag\" within each group\n",
    "# Compare current 2h activity to group's typical 2h activity (rolling mean/std over last 30 days)\n",
    "# If you don't have a full year in every group, this still behaves fine due to min_periods.\n",
    "BASELINE = \"30d\"\n",
    "grp_roll_mean = (\n",
    "    df.groupby(\"group_id\")[\"posts_prev_2h_group\"]\n",
    "      .rolling(BASELINE, min_periods=50)\n",
    "      .mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "grp_roll_std = (\n",
    "    df.groupby(\"group_id\")[\"posts_prev_2h_group\"]\n",
    "      .rolling(BASELINE, min_periods=50)\n",
    "      .std()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "df[\"burst_z_group\"] = (df[\"posts_prev_2h_group\"] - grp_roll_mean) / (grp_roll_std + 1e-6)\n",
    "df[\"burst_z_group\"] = df[\"burst_z_group\"].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "# Flag: 1 means \"unusually busy right now\"\n",
    "df[\"burst_flag_group\"] = (df[\"burst_z_group\"] >= 1.0).astype(int)\n",
    "\n",
    "# Return to normal index for saving\n",
    "df = df.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "affa7033-6d4b-4290-a7f0-3b6007e8dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../yikyak_metadata.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (CSC371)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
