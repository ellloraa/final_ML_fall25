{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "120e45c1-7f84-46c4-8655-5967f37236c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1pkm0v2', 'text': 'Majority of journalists killed in Gaza linked to terror organizations, study says', 'created_at': '2025-12-12T07:33:45Z', 'vote_total': 0, 'comment_count': 48, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'YR2Fuc7h'}\n",
      "{'id': '1pkqfa5', 'text': 'Israel to review reports that troops killed three-year-old in Gaza', 'created_at': '2025-12-12T12:15:23Z', 'vote_total': 2, 'comment_count': 34, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'CuzDBV8F'}\n",
      "{'id': '1pkkct8', 'text': 'Flood misery for Gazans awaiting next stage of peace plan', 'created_at': '2025-12-12T05:53:41Z', 'vote_total': 0, 'comment_count': 14, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'SxYD3jP4'}\n",
      "{'id': '1pkuibp', 'text': 'EU in ‘chaos’: Nawrocki says Poland’s security strategy should align with US', 'created_at': '2025-12-12T15:19:56Z', 'vote_total': 0, 'comment_count': 14, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'HL9aDmRb'}\n",
      "{'id': '1pkqwcq', 'text': 'King to share personal cancer update in campaign video message', 'created_at': '2025-12-12T12:40:55Z', 'vote_total': 9, 'comment_count': 30, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'ceUAnu7j'}\n",
      "{'id': '1pkyzxe', 'text': 'Israel refuses to withdraw from Syria', 'created_at': '2025-12-12T18:15:59Z', 'vote_total': 46, 'comment_count': 45, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'Fxqn5oDA'}\n",
      "{'id': '1pkyuq6', 'text': 'Russia files lawsuit against Euroclear as Europe bickers over frozen assets', 'created_at': '2025-12-12T18:10:19Z', 'vote_total': 0, 'comment_count': 4, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'xnXTbgUq'}\n",
      "{'id': '1pl0xkp', 'text': 'UAE makes major $550 million early pledge to the UN’s 2026 humanitarian appeal', 'created_at': '2025-12-12T19:33:29Z', 'vote_total': 0, 'comment_count': 1, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'YqlWGrAE'}\n",
      "{'id': '1pkmyjl', 'text': 'Defence experts term Russia India’s most reliable military partner', 'created_at': '2025-12-12T08:36:38Z', 'vote_total': 29, 'comment_count': 28, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': '67q3a0I2'}\n",
      "{'id': '1pkpnkm', 'text': 'Russian central bank to sue Euroclear | VRT NWS: news', 'created_at': '2025-12-12T11:32:10Z', 'vote_total': 19, 'comment_count': 33, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'ni8JkMxW'}\n",
      "{'id': '1pkriiu', 'text': \"Majority of young American adults 'active in Hamas resistance,' Khaled Mashaal claims\", 'created_at': '2025-12-12T13:11:18Z', 'vote_total': 0, 'comment_count': 10, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'jhLilvtU'}\n",
      "{'id': '1pkfps1', 'text': 'Finally in Oslo, Nobel laureate Machado vows to return to Venezuela', 'created_at': '2025-12-12T02:02:07Z', 'vote_total': 63, 'comment_count': 46, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'WV1XYhaD'}\n",
      "{'id': '1pl20qm', 'text': 'Iran-backed militias reassert power in Iraq, proving the Islamic axis is still standing', 'created_at': '2025-12-12T20:18:11Z', 'vote_total': 5, 'comment_count': 1, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'NyfQFhuC'}\n",
      "{'id': '1pkz6g0', 'text': 'Trump says Thailand, Cambodia agree to renew ceasefire after deadly clashes', 'created_at': '2025-12-12T18:23:05Z', 'vote_total': 14, 'comment_count': 22, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'Vo0B5z8I'}\n",
      "{'id': '1pkpwl0', 'text': \"Schoolboys to be target of UK's violence against women strategy\", 'created_at': '2025-12-12T11:47:13Z', 'vote_total': 144, 'comment_count': 169, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'JZWskOS1'}\n",
      "{'id': '1pkshy4', 'text': \"Sam Altmans 'World' launches its Super App\", 'created_at': '2025-12-12T13:56:35Z', 'vote_total': 0, 'comment_count': 15, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'ppzruS4R'}\n",
      "{'id': '1pkmha9', 'text': \"Ethiopia TikTok: Six influencers arrested for 'indecent' dress at awards ceremony\", 'created_at': '2025-12-12T08:04:05Z', 'vote_total': 30, 'comment_count': 22, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'kQAfKdxp'}\n",
      "{'id': '1pl12yf', 'text': 'Trump says Thailand and Cambodia agree to end fighting', 'created_at': '2025-12-12T19:39:39Z', 'vote_total': 0, 'comment_count': 22, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': '0d20xpru'}\n",
      "{'id': '1pkp9uf', 'text': 'Milei wants to define governance ground rules with ‘fiscal stability bill’', 'created_at': '2025-12-12T11:09:01Z', 'vote_total': 18, 'comment_count': 21, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': '9XkyiR1o'}\n",
      "{'id': '1pl2fj1', 'text': 'Israel and Qatar hold secret meeting in New York to rebuild ties', 'created_at': '2025-12-12T20:35:29Z', 'vote_total': 14, 'comment_count': 4, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': '1Umwklrw'}\n",
      "{'id': '1pkc59g', 'text': 'EU triggers emergency clause to indefinitely immobilise Russian assets', 'created_at': '2025-12-11T23:17:23Z', 'vote_total': 19886, 'comment_count': 520, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'o5sDy7c9'}\n",
      "{'id': '1pkp1cj', 'text': \"Only eight countries, including the US, Russia and China, opposed Ukraine's resolution condemning Russia's suicide drone attack on the Chernobyl sarcophagus.\", 'created_at': '2025-12-12T10:54:39Z', 'vote_total': 13938, 'comment_count': 714, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'hhPDJTTf'}\n",
      "{'id': '1pkuua1', 'text': 'Putin Invites Journalists to\\xa0“Surrounded” Ukrainian Forces in\\xa0Kupiansk—Zelenskyy Shows Up\\xa0Instead', 'created_at': '2025-12-12T15:33:10Z', 'vote_total': 13125, 'comment_count': 423, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'gAdkbVBc'}\n",
      "{'id': '1pkf6d1', 'text': '[Canada] Another Conservative crosses the floor, bringing Liberals 1 MP shy of majority', 'created_at': '2025-12-12T01:36:22Z', 'vote_total': 8416, 'comment_count': 772, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'FYGJRilj'}\n",
      "{'id': '1pkf9pv', 'text': 'US sides with Russia on UN resolution on Chornobyl disaster', 'created_at': '2025-12-12T01:40:55Z', 'vote_total': 5955, 'comment_count': 606, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'nW4t8zfU'}\n",
      "{'id': '1pktbuq', 'text': 'Germany accuses Russia of cyber attack and election disinformation campaign', 'created_at': '2025-12-12T14:31:52Z', 'vote_total': 5229, 'comment_count': 91, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'ZSMDOGL5'}\n",
      "{'id': '1pkm4wv', 'text': 'Meta shuts down global accounts linked to abortion advice and queer content', 'created_at': '2025-12-12T07:41:21Z', 'vote_total': 5091, 'comment_count': 275, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'i75PHDsN'}\n",
      "{'id': '1pkir8e', 'text': \"UK 'rapidly developing' plans to prepare for war, says armed forces minister\", 'created_at': '2025-12-12T04:27:14Z', 'vote_total': 4331, 'comment_count': 702, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'AR2E7Fr5'}\n",
      "{'id': '1pkm3jq', 'text': 'Zelenskyy: US wants Ukraine to withdraw from Donetsk for vague “special economic zone” – Kyiv may refuse', 'created_at': '2025-12-12T07:38:42Z', 'vote_total': 3184, 'comment_count': 219, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'h5XWi2wh'}\n",
      "{'id': '1pkocb5', 'text': 'For 1st time since trade war, Canada exported more than it imported', 'created_at': '2025-12-12T10:09:59Z', 'vote_total': 2769, 'comment_count': 243, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'TwlrzbeT'}\n",
      "{'id': '1pklrif', 'text': \"Russia sending Ukrainian children to 'harmful and abusive' camp in North Korea, says human rights group\", 'created_at': '2025-12-12T07:16:53Z', 'vote_total': 2606, 'comment_count': 63, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'EwCKbC29'}\n",
      "{'id': '1pkr9wh', 'text': 'Ukrainian forces push Russians out of northwestern Kupiansk, war monitors say', 'created_at': '2025-12-12T13:00:00Z', 'vote_total': 1807, 'comment_count': 25, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': '7l4Xyo2G'}\n",
      "{'id': '1pl2fq2', 'text': 'US House approves defense bill granting Ukraine $400 million a year in military aid through 2027', 'created_at': '2025-12-12T20:35:43Z', 'vote_total': 1568, 'comment_count': 103, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': '1aTbUhlG'}\n",
      "{'id': '1pkw2u6', 'text': '‘Reality speaks for itself’ — From war-torn Kupiansk, Zelensky praises troops after reports of successful Ukrainian counterattack', 'created_at': '2025-12-12T16:22:27Z', 'vote_total': 1474, 'comment_count': 18, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'y5SIlShR'}\n",
      "{'id': '1pksu8p', 'text': 'EU decides to impose €3 tax on small parcels to tackle Chinese imports', 'created_at': '2025-12-12T14:11:05Z', 'vote_total': 1314, 'comment_count': 230, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': '2DW9dqe9'}\n",
      "{'id': '1pkrs7x', 'text': \"Ukraine and Europe hand US revisions of peace plan as Trump grows 'sick of meetings'\", 'created_at': '2025-12-12T13:24:09Z', 'vote_total': 1114, 'comment_count': 194, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'imWVz2q4'}\n",
      "{'id': '1pkub71', 'text': 'Canadian boycott of U.S. hitting border states hard: Congressional report', 'created_at': '2025-12-12T15:11:56Z', 'vote_total': 1029, 'comment_count': 344, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'CUZ5hOZ5'}\n",
      "{'id': '1pkhppn', 'text': 'Magnitude 6.7 earthquake hits Japan’s northeast, tsunami warning issued', 'created_at': '2025-12-12T03:36:18Z', 'vote_total': 997, 'comment_count': 66, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'ncyrOAoe'}\n",
      "{'id': '1pl0hi6', 'text': 'Danish intelligence accuses US of using economic power to ‘assert its will’ over allies', 'created_at': '2025-12-12T19:15:08Z', 'vote_total': 829, 'comment_count': 43, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'Mhw81uVC'}\n",
      "{'id': '1pkojl4', 'text': 'Croatia purchases German-made Leopards after donating tanks to Ukraine', 'created_at': '2025-12-12T10:23:02Z', 'vote_total': 714, 'comment_count': 13, 'alias': 'Anonymous', 'group_id': 'cceeb7e5-97ef-46d3-b8d9-83860bf41387', 'index_code': 'f4mfSoME'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import uuid\n",
    "import random\n",
    "import string\n",
    "from datetime import datetime\n",
    "\n",
    "# Helper: generate random short code\n",
    "def random_index_code(length=8):\n",
    "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
    "\n",
    "# Transform Reddit post into YikYak-style schema\n",
    "def reddit_to_yikyak(post, group_id=None):\n",
    "    return {\n",
    "        \"id\": post[\"id\"],\n",
    "        \"text\": f'{post.get(\"title\",\"\")} {post.get(\"selftext\",\"\")}'.strip(),\n",
    "        \"created_at\": datetime.utcfromtimestamp(post[\"created_utc\"]).isoformat() + \"Z\",\n",
    "        \"vote_total\": post.get(\"ups\", 0),\n",
    "        \"comment_count\": post.get(\"num_comments\", 0),\n",
    "        \"alias\": \"Anonymous\",\n",
    "        \"group_id\": group_id if group_id else str(uuid.uuid4()),\n",
    "        \"index_code\": random_index_code()\n",
    "    }\n",
    "\n",
    "# Fetch posts from a subreddit listing\n",
    "def fetch_reddit_posts(subreddit, listing=\"controversial\", limit=50, time=\"day\"):\n",
    "    url = f\"https://www.reddit.com/r/{subreddit}/{listing}.json\"\n",
    "    params = {\"limit\": limit, \"t\": time}\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; DataScraper/1.0)\"}\n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    return [child[\"data\"] for child in data[\"data\"][\"children\"]]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    subreddit = \"worldnews\"\n",
    "\n",
    "    # Fetch controversial and top posts\n",
    "    controversial_posts = fetch_reddit_posts(subreddit, \"controversial\", limit=20, time=\"day\")\n",
    "    top_posts = fetch_reddit_posts(subreddit, \"top\", limit=20, time=\"day\")\n",
    "\n",
    "    # Clean them into YikYak schema\n",
    "    group_id = str(uuid.uuid4())  # one group ID for this batch\n",
    "    dataset = [reddit_to_yikyak(p, group_id) for p in controversial_posts + top_posts]\n",
    "\n",
    "    # Print or save\n",
    "    for entry in dataset:\n",
    "        print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b85b1628-c750-4bb7-a9f3-9e7f3e5bc1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching controversial from worldnews: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/worldnews/controversial.json?limit=50&t=day\n",
      "Error fetching top from worldnews: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/worldnews/top.json?limit=50&t=day\n",
      "Error fetching hot from worldnews: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/worldnews/hot.json?limit=50&t=day\n",
      "Error fetching new from worldnews: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/worldnews/new.json?limit=50&t=day\n",
      "Error fetching controversial from news: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/news/controversial.json?limit=50&t=day\n",
      "Error fetching top from news: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/news/top.json?limit=50&t=day\n",
      "Error fetching hot from news: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/news/hot.json?limit=50&t=day\n",
      "Saved 1544 text-only posts to reddit_yikyak_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import uuid\n",
    "import random\n",
    "import string\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Helper: generate random short code\n",
    "def random_index_code(length=8):\n",
    "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
    "\n",
    "# Helper: check if post is text-only (skip images/videos)\n",
    "def is_text_post(post):\n",
    "    if post.get(\"is_video\"):\n",
    "        return False\n",
    "    if post.get(\"post_hint\") in [\"image\", \"video\"]:\n",
    "        return False\n",
    "    if \"preview\" in post:  # images usually have a preview\n",
    "        return False\n",
    "    if str(post.get(\"url_overridden_by_dest\", \"\")).lower().endswith((\".jpg\", \".jpeg\", \".png\", \".gif\")):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Transform Reddit post into YikYak-style schema + controversial flag\n",
    "def reddit_to_yikyak(post, listing, group_id=None):\n",
    "    if not is_text_post(post):\n",
    "        return None  # skip non-text posts\n",
    "    return {\n",
    "        \"id\": post[\"id\"],\n",
    "        \"text\": f'{post.get(\"title\",\"\")} {post.get(\"selftext\",\"\")}'.strip(),\n",
    "        \"created_at\": datetime.utcfromtimestamp(post[\"created_utc\"]).isoformat() + \"Z\",\n",
    "        \"vote_total\": post.get(\"ups\", 0),\n",
    "        \"comment_count\": post.get(\"num_comments\", 0),\n",
    "        \"alias\": \"Anonymous\",\n",
    "        \"group_id\": group_id if group_id else str(uuid.uuid4()),\n",
    "        \"index_code\": random_index_code(),\n",
    "        \"controversial_flag\": 1 if listing == \"controversial\" else 0\n",
    "    }\n",
    "\n",
    "# Fetch posts from a subreddit listing\n",
    "def fetch_reddit_posts(subreddit, listing=\"controversial\", limit=50, time=\"day\"):\n",
    "    url = f\"https://www.reddit.com/r/{subreddit}/{listing}.json\"\n",
    "    params = {\"limit\": limit, \"t\": time}\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; DataScraper/1.0)\"}\n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    return [child[\"data\"] for child in data[\"data\"][\"children\"]]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Pick ~20 top subreddits (you can adjust this list)\n",
    "    subreddits = [\n",
    "        \"worldnews\", \"news\", \"politics\", \"technology\", \"science\",\n",
    "        \"gaming\", \"movies\", \"music\", \"sports\", \"books\",\n",
    "        \"askreddit\", \"funny\", \"pics\", \"todayilearned\", \"dataisbeautiful\",\n",
    "        \"art\", \"history\", \"space\", \"explainlikeimfive\", \"economics\",\n",
    "        \"popculturechat\", \"mildlyinfuriating\", \"interesting\"\n",
    "    ]\n",
    "\n",
    "    listings = [\"controversial\", \"top\", \"hot\", \"new\"]\n",
    "    dataset = []\n",
    "\n",
    "    for subreddit in subreddits:\n",
    "        group_id = str(uuid.uuid4())  # one group ID per subreddit batch\n",
    "        for listing in listings:\n",
    "            try:\n",
    "                posts = fetch_reddit_posts(subreddit, listing, limit=50, time=\"day\")\n",
    "                for p in posts:\n",
    "                    transformed = reddit_to_yikyak(p, listing, group_id)\n",
    "                    if transformed:  # only add text posts\n",
    "                        dataset.append(transformed)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {listing} from {subreddit}: {e}\")\n",
    "\n",
    "    # Save to JSON file\n",
    "    with open(\"reddit_yikyak_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Saved {len(dataset)} text-only posts to reddit_yikyak_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e32d9e8-d620-4996-bacd-e86001f30491",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [\"AskReddit\", \"OutOfTheLoop\", \"ExplainItLikeImFive\", \"AskUK\", \"AskMen\"]\n",
    "B = [\"worldnews\", \"UnderReportedNews\", \"politics\"]\n",
    "C = [\"todayilearned\", \"science\", \"technology\", \"interesting\"]\n",
    "D = [\"movies\", \"television\", \"Music\", \"popculturechat\", \"Fauxmoi\"]\n",
    "E = [\"AITAH\", \"AmItheAsshole\", \"tifu\", \"antiwork\", \"recruitinghell\", \"jobs\", \"complaints\"]\n",
    "F = [\"Piracy\", \"KitchenConfidential\", \"buildapc\", \"nfl\", \"formula1\", \"sports\", \"travel\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee0be686-5007-4d3b-8fe1-1e411f4cb702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, uuid, random, string, json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def random_index_code(length=8):\n",
    "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
    "\n",
    "def is_text_post(post):\n",
    "    if post.get(\"is_video\"):\n",
    "        return False\n",
    "    if post.get(\"post_hint\") in [\"image\", \"video\", \"hosted:video\", \"rich:video\"]:\n",
    "        return False\n",
    "    if \"preview\" in post:  # common signal for images\n",
    "        return False\n",
    "    url = str(post.get(\"url_overridden_by_dest\", \"\")).lower()\n",
    "    if url.endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\")):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def reddit_to_yikyak(post, listing, group_id=None):\n",
    "    if not is_text_post(post):\n",
    "        return None\n",
    "    return {\n",
    "        \"id\": post[\"id\"],\n",
    "        \"text\": f'{post.get(\"title\",\"\")} {post.get(\"selftext\",\"\")}'.strip(),\n",
    "        \"created_at\": datetime.utcfromtimestamp(post[\"created_utc\"]).isoformat() + \"Z\",\n",
    "        \"vote_total\": post.get(\"ups\", 0),\n",
    "        \"comment_count\": post.get(\"num_comments\", 0),\n",
    "        \"alias\": \"Anonymous\",\n",
    "        \"group_id\": group_id if group_id else str(uuid.uuid4()),\n",
    "        \"index_code\": random_index_code(),\n",
    "        \"controversial_flag\": 1 if listing == \"controversial\" else 0\n",
    "    }\n",
    "\n",
    "def get_with_backoff(session, url, params, headers, max_retries=6, base_sleep=2.0):\n",
    "    \"\"\"\n",
    "    - Sleeps a bit between requests (base_sleep)\n",
    "    - On 429, respects Retry-After if present, otherwise exponential backoff\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        time.sleep(base_sleep)  # steady throttle for every request\n",
    "        resp = session.get(url, params=params, headers=headers, timeout=30)\n",
    "\n",
    "        if resp.status_code != 429:\n",
    "            resp.raise_for_status()\n",
    "            return resp\n",
    "\n",
    "        # 429\n",
    "        attempt += 1\n",
    "        if attempt > max_retries:\n",
    "            resp.raise_for_status()\n",
    "\n",
    "        retry_after = resp.headers.get(\"Retry-After\")\n",
    "        if retry_after is not None:\n",
    "            sleep_s = float(retry_after) + 1.0\n",
    "        else:\n",
    "            sleep_s = (2 ** attempt) + random.random()  # exp backoff + jitter\n",
    "\n",
    "        print(f\"429 hit. Sleeping {sleep_s:.1f}s then retrying...\")\n",
    "        time.sleep(sleep_s)\n",
    "\n",
    "def fetch_reddit_posts(session, subreddit, listing=\"hot\", limit=25, time_filter=\"day\", after=None):\n",
    "    url = f\"https://www.reddit.com/r/{subreddit}/{listing}.json\"\n",
    "    params = {\"limit\": limit}\n",
    "    if listing in (\"top\", \"controversial\"):\n",
    "        params[\"t\"] = time_filter\n",
    "    if after:\n",
    "        params[\"after\"] = after\n",
    "\n",
    "    headers = {\"User-Agent\": \"yikyak-dataset-bot/0.1 (by u/yourusername)\"}\n",
    "    resp = get_with_backoff(session, url, params=params, headers=headers)\n",
    "    data = resp.json()\n",
    "    children = [c[\"data\"] for c in data[\"data\"][\"children\"]]\n",
    "    after = data[\"data\"].get(\"after\")\n",
    "    return children, after\n",
    "\n",
    "def scrape_group(\n",
    "    subreddits,\n",
    "    listings=(\"controversial\", \"top\", \"hot\", \"new\"),\n",
    "    per_request_limit=25,\n",
    "    max_posts_per_listing=50,   # paginate up to this many per listing\n",
    "    time_filter=\"day\",\n",
    "    base_sleep=2.0,\n",
    "    out_dir=\"reddit_batches\",\n",
    "    out_name=\"batch.json\",\n",
    "):\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    dataset = []\n",
    "    session = requests.Session()\n",
    "\n",
    "    for subreddit in subreddits:\n",
    "        group_id = str(uuid.uuid4())\n",
    "        for listing in listings:\n",
    "            collected = 0\n",
    "            after = None\n",
    "            while collected < max_posts_per_listing:\n",
    "                posts, after = fetch_reddit_posts(\n",
    "                    session,\n",
    "                    subreddit=subreddit,\n",
    "                    listing=listing,\n",
    "                    limit=per_request_limit,\n",
    "                    time_filter=time_filter,\n",
    "                    after=after,\n",
    "                )\n",
    "\n",
    "                for p in posts:\n",
    "                    item = reddit_to_yikyak(p, listing, group_id)\n",
    "                    if item:\n",
    "                        dataset.append(item)\n",
    "\n",
    "                collected += len(posts)\n",
    "                if not after:\n",
    "                    break  # no more pages\n",
    "\n",
    "            # extra pause between listings (helps a lot)\n",
    "            time.sleep(base_sleep)\n",
    "\n",
    "    out_path = Path(out_dir) / out_name\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Saved {len(dataset)} text-only posts to {out_path}\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4507dd72-be70-4a90-941e-729b46f7aaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 hit. Sleeping 1.0s then retrying...\n",
      "429 hit. Sleeping 1.0s then retrying...\n",
      "429 hit. Sleeping 1.0s then retrying...\n",
      "429 hit. Sleeping 1.0s then retrying...\n",
      "429 hit. Sleeping 1.0s then retrying...\n",
      "429 hit. Sleeping 1.0s then retrying...\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://www.reddit.com/r/AskReddit/controversial.json?limit=25&t=day",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataA \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_request_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_posts_per_listing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_sleep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_A.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 98\u001b[0m, in \u001b[0;36mscrape_group\u001b[0;34m(subreddits, listings, per_request_limit, max_posts_per_listing, time_filter, base_sleep, out_dir, out_name)\u001b[0m\n\u001b[1;32m     96\u001b[0m after \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m collected \u001b[38;5;241m<\u001b[39m max_posts_per_listing:\n\u001b[0;32m---> 98\u001b[0m     posts, after \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_reddit_posts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubreddit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubreddit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlisting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlisting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mper_request_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mafter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mafter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m posts:\n\u001b[1;32m    108\u001b[0m         item \u001b[38;5;241m=\u001b[39m reddit_to_yikyak(p, listing, group_id)\n",
      "Cell \u001b[0;32mIn[11], line 72\u001b[0m, in \u001b[0;36mfetch_reddit_posts\u001b[0;34m(session, subreddit, listing, limit, time_filter, after)\u001b[0m\n\u001b[1;32m     69\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m after\n\u001b[1;32m     71\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myikyak-dataset-bot/0.1 (by u/yourusername)\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m---> 72\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mget_with_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m data \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     74\u001b[0m children \u001b[38;5;241m=\u001b[39m [c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchildren\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "Cell \u001b[0;32mIn[11], line 52\u001b[0m, in \u001b[0;36mget_with_backoff\u001b[0;34m(session, url, params, headers, max_retries, base_sleep)\u001b[0m\n\u001b[1;32m     50\u001b[0m attempt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attempt \u001b[38;5;241m>\u001b[39m max_retries:\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m retry_after \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetry-After\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retry_after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/pub/envs/CSC371-shared/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/AskReddit/controversial.json?limit=25&t=day"
     ]
    }
   ],
   "source": [
    "dataA = scrape_group(\n",
    "    A,\n",
    "    per_request_limit=25,\n",
    "    max_posts_per_listing=50,\n",
    "    base_sleep=2.5,\n",
    "    out_name=\"batch_A.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb24099-9839-4315-b96f-e22fcc623707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 hit. Sleeping 1.0s then retrying...\n",
      "429 hit. Sleeping 1.0s then retrying...\n",
      "429 hit. Sleeping 1.0s then retrying...\n",
      "429 hit. Sleeping 1.0s then retrying...\n",
      "429 hit. Sleeping 1.0s then retrying...\n",
      "429 hit. Sleeping 1.0s then retrying...\n"
     ]
    }
   ],
   "source": [
    "dataB = scrape_group(\n",
    "    B,\n",
    "    per_request_limit=25,\n",
    "    max_posts_per_listing=50,\n",
    "    base_sleep=2.5,\n",
    "    out_name=\"batch_B.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fed90f-22e3-4054-87c4-e67677297e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataC = scrape_group(\n",
    "    C,\n",
    "    per_request_limit=25,\n",
    "    max_posts_per_listing=50,\n",
    "    base_sleep=2.5,\n",
    "    out_name=\"batch_C.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e98601-0a0e-4843-a1a1-8448deda0e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataD = scrape_group(\n",
    "    D,\n",
    "    per_request_limit=25,\n",
    "    max_posts_per_listing=50,\n",
    "    base_sleep=2.5,\n",
    "    out_name=\"batch_D.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc8a3e9-cb27-4465-8f2a-4371ac80a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataE = scrape_group(\n",
    "    E,\n",
    "    per_request_limit=25,\n",
    "    max_posts_per_listing=50,\n",
    "    base_sleep=2.5,\n",
    "    out_name=\"batch_E.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae4539-208d-4346-86b7-7cc07079a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF = scrape_group(\n",
    "    F,\n",
    "    per_request_limit=25,\n",
    "    max_posts_per_listing=50,\n",
    "    base_sleep=2.5,\n",
    "    out_name=\"batch_F.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f54ba4a-6b20-4b65-bf2b-6ce356df9d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4191 existing items from merged_file.json\n",
      "Added 828 items (no dedupe).\n",
      "Merged file now has 5019 items: merged_file.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import requests\n",
    "import uuid\n",
    "import random\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "MERGED_PATH = \"merged_file.json\"\n",
    "\n",
    "def random_index_code(length=8):\n",
    "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
    "\n",
    "def is_text_post(post):\n",
    "    if post.get(\"is_video\"):\n",
    "        return False\n",
    "    if post.get(\"post_hint\") in [\"image\", \"video\", \"hosted:video\", \"rich:video\"]:\n",
    "        return False\n",
    "    if \"preview\" in post:\n",
    "        return False\n",
    "    if str(post.get(\"url_overridden_by_dest\", \"\")).lower().endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\")):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def reddit_to_yikyak(post, listing, group_id=None):\n",
    "    if not is_text_post(post):\n",
    "        return None\n",
    "    return {\n",
    "        \"id\": post[\"id\"],\n",
    "        \"text\": f'{post.get(\"title\",\"\")} {post.get(\"selftext\",\"\")}'.strip(),\n",
    "        \"created_at\": datetime.utcfromtimestamp(post[\"created_utc\"]).isoformat() + \"Z\",\n",
    "        \"vote_total\": post.get(\"ups\", 0),\n",
    "        \"comment_count\": post.get(\"num_comments\", 0),\n",
    "        \"alias\": \"Anonymous\",\n",
    "        \"group_id\": group_id if group_id else str(uuid.uuid4()),\n",
    "        \"index_code\": random_index_code(),\n",
    "        \"controversial_flag\": 1 if listing == \"controversial\" else 0\n",
    "    }\n",
    "\n",
    "def fetch_reddit_posts(subreddit, listing=\"controversial\", limit=50, time_filter=\"day\"):\n",
    "    url = f\"https://www.reddit.com/r/{subreddit}/{listing}.json\"\n",
    "    params = {\"limit\": limit}\n",
    "    if listing in (\"controversial\", \"top\"):\n",
    "        params[\"t\"] = time_filter\n",
    "    headers = {\"User-Agent\": \"yikyak-dataset-bot/0.1 (by u/yourusername)\"}\n",
    "    r = requests.get(url, params=params, headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return [child[\"data\"] for child in data[\"data\"][\"children\"]]\n",
    "\n",
    "def load_merged(path):\n",
    "    if not os.path.exists(path):\n",
    "        return []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            return data if isinstance(data, list) else []\n",
    "        except json.JSONDecodeError:\n",
    "            return []\n",
    "\n",
    "def save_merged(path, items):\n",
    "    tmp = path + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(items, f, indent=2, ensure_ascii=False)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    subreddits = [\"askscience\", \"cfb\", \"dating_advice\", \"overheard\", \"mac\", \"changemyview\", \"rareinsults\", \"snorkblot\"]    \n",
    "    listings = [\"controversial\", \"top\", \"hot\", \"new\"]\n",
    "\n",
    "    merged = load_merged(MERGED_PATH)\n",
    "    print(f\"Loaded {len(merged)} existing items from {MERGED_PATH}\")\n",
    "\n",
    "    new_items = []\n",
    "    for subreddit in subreddits:\n",
    "        group_id = str(uuid.uuid4())\n",
    "        for listing in listings:\n",
    "            try:\n",
    "                posts = fetch_reddit_posts(subreddit, listing, limit=50, time_filter=\"day\")\n",
    "                for p in posts:\n",
    "                    transformed = reddit_to_yikyak(p, listing, group_id)\n",
    "                    if transformed:\n",
    "                        new_items.append(transformed)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {listing} from {subreddit}: {e}\")\n",
    "\n",
    "    merged.extend(new_items)\n",
    "    save_merged(MERGED_PATH, merged)\n",
    "\n",
    "    with open(\"new_items_this_run.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(new_items, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Added {len(new_items)} items (no dedupe).\")\n",
    "    print(f\"Merged file now has {len(merged)} items: {MERGED_PATH}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb179570-b979-4234-bce0-d4bcf6ef15fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5019 existing items from merged_file.json\n",
      "Added 922 items (no dedupe).\n",
      "Merged file now has 5941 items: merged_file.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import requests\n",
    "import uuid\n",
    "import random\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "MERGED_PATH = \"merged_file.json\"\n",
    "\n",
    "def random_index_code(length=8):\n",
    "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
    "\n",
    "def is_text_post(post):\n",
    "    if post.get(\"is_video\"):\n",
    "        return False\n",
    "    if post.get(\"post_hint\") in [\"image\", \"video\", \"hosted:video\", \"rich:video\"]:\n",
    "        return False\n",
    "    if \"preview\" in post:\n",
    "        return False\n",
    "    if str(post.get(\"url_overridden_by_dest\", \"\")).lower().endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\")):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def reddit_to_yikyak(post, listing, group_id=None):\n",
    "    if not is_text_post(post):\n",
    "        return None\n",
    "    return {\n",
    "        \"id\": post[\"id\"],\n",
    "        \"text\": f'{post.get(\"title\",\"\")} {post.get(\"selftext\",\"\")}'.strip(),\n",
    "        \"created_at\": datetime.utcfromtimestamp(post[\"created_utc\"]).isoformat() + \"Z\",\n",
    "        \"vote_total\": post.get(\"ups\", 0),\n",
    "        \"comment_count\": post.get(\"num_comments\", 0),\n",
    "        \"alias\": \"Anonymous\",\n",
    "        \"group_id\": group_id if group_id else str(uuid.uuid4()),\n",
    "        \"index_code\": random_index_code(),\n",
    "        \"controversial_flag\": 1 if listing == \"controversial\" else 0\n",
    "    }\n",
    "\n",
    "def fetch_reddit_posts(subreddit, listing=\"controversial\", limit=50, time_filter=\"day\"):\n",
    "    url = f\"https://www.reddit.com/r/{subreddit}/{listing}.json\"\n",
    "    params = {\"limit\": limit}\n",
    "    if listing in (\"controversial\", \"top\"):\n",
    "        params[\"t\"] = time_filter\n",
    "    headers = {\"User-Agent\": \"yikyak-dataset-bot/0.1 (by u/yourusername)\"}\n",
    "    r = requests.get(url, params=params, headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return [child[\"data\"] for child in data[\"data\"][\"children\"]]\n",
    "\n",
    "def load_merged(path):\n",
    "    if not os.path.exists(path):\n",
    "        return []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            return data if isinstance(data, list) else []\n",
    "        except json.JSONDecodeError:\n",
    "            return []\n",
    "\n",
    "def save_merged(path, items):\n",
    "    tmp = path + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(items, f, indent=2, ensure_ascii=False)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    subreddits = [\"privacy\", \"mildlyinteresting\", \"dating\", \"askanamerican\", \"moviesuggestions\", \"northcarolina\", \"openai\", \"webdev\", \"iama\"]\n",
    "    listings = [\"controversial\", \"top\", \"hot\", \"new\"]\n",
    "\n",
    "    merged = load_merged(MERGED_PATH)\n",
    "    print(f\"Loaded {len(merged)} existing items from {MERGED_PATH}\")\n",
    "\n",
    "    new_items = []\n",
    "    for subreddit in subreddits:\n",
    "        group_id = str(uuid.uuid4())\n",
    "        for listing in listings:\n",
    "            try:\n",
    "                posts = fetch_reddit_posts(subreddit, listing, limit=50, time_filter=\"day\")\n",
    "                for p in posts:\n",
    "                    transformed = reddit_to_yikyak(p, listing, group_id)\n",
    "                    if transformed:\n",
    "                        new_items.append(transformed)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {listing} from {subreddit}: {e}\")\n",
    "\n",
    "    merged.extend(new_items)\n",
    "    save_merged(MERGED_PATH, merged)\n",
    "\n",
    "    with open(\"new_items_this_run.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(new_items, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Added {len(new_items)} items (no dedupe).\")\n",
    "    print(f\"Merged file now has {len(merged)} items: {MERGED_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93a401ae-5f5c-4233-a2b4-8c317dc1b398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5941 existing items from merged_file.json\n",
      "Error fetching controversial from ama: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/ama/controversial.json?limit=50&t=day\n",
      "Error fetching top from ama: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/ama/top.json?limit=50&t=day\n",
      "Error fetching hot from ama: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/ama/hot.json?limit=50\n",
      "Error fetching new from ama: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/ama/new.json?limit=50\n",
      "Error fetching controversial from casualconversation: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/casualconversation/controversial.json?limit=50&t=day\n",
      "Error fetching top from casualconversation: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/casualconversation/top.json?limit=50&t=day\n",
      "Error fetching hot from casualconversation: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/casualconversation/hot.json?limit=50\n",
      "Error fetching new from casualconversation: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/casualconversation/new.json?limit=50\n",
      "Error fetching controversial from twohottakes: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/twohottakes/controversial.json?limit=50&t=day\n",
      "Error fetching top from twohottakes: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/twohottakes/top.json?limit=50&t=day\n",
      "Error fetching hot from twohottakes: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/twohottakes/hot.json?limit=50\n",
      "Error fetching new from twohottakes: 429 Client Error: Too Many Requests for url: https://www.reddit.com/r/twohottakes/new.json?limit=50\n",
      "Added 1260 items (no dedupe).\n",
      "Merged file now has 7201 items: merged_file.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import uuid\n",
    "import random\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "MERGED_PATH = \"merged_file.json\"\n",
    "\n",
    "def random_index_code(length=8):\n",
    "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
    "\n",
    "def is_text_post(post):\n",
    "    if post.get(\"is_video\"):\n",
    "        return False\n",
    "    if post.get(\"post_hint\") in [\"image\", \"video\", \"hosted:video\", \"rich:video\"]:\n",
    "        return False\n",
    "    if \"preview\" in post:\n",
    "        return False\n",
    "    if str(post.get(\"url_overridden_by_dest\", \"\")).lower().endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\")):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def reddit_to_yikyak(post, listing, group_id=None):\n",
    "    if not is_text_post(post):\n",
    "        return None\n",
    "    return {\n",
    "        \"id\": post[\"id\"],\n",
    "        \"text\": f'{post.get(\"title\",\"\")} {post.get(\"selftext\",\"\")}'.strip(),\n",
    "        \"created_at\": datetime.utcfromtimestamp(post[\"created_utc\"]).isoformat() + \"Z\",\n",
    "        \"vote_total\": post.get(\"ups\", 0),\n",
    "        \"comment_count\": post.get(\"num_comments\", 0),\n",
    "        \"alias\": \"Anonymous\",\n",
    "        \"group_id\": group_id if group_id else str(uuid.uuid4()),\n",
    "        \"index_code\": random_index_code(),\n",
    "        \"controversial_flag\": 1 if listing == \"controversial\" else 0\n",
    "    }\n",
    "\n",
    "def fetch_reddit_posts(subreddit, listing=\"controversial\", limit=50, time_filter=\"day\"):\n",
    "    url = f\"https://www.reddit.com/r/{subreddit}/{listing}.json\"\n",
    "    params = {\"limit\": limit}\n",
    "    if listing in (\"controversial\", \"top\"):\n",
    "        params[\"t\"] = time_filter\n",
    "    headers = {\"User-Agent\": \"yikyak-dataset-bot/0.1 (by u/yourusername)\"}\n",
    "    r = requests.get(url, params=params, headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return [child[\"data\"] for child in data[\"data\"][\"children\"]]\n",
    "\n",
    "def load_merged(path):\n",
    "    if not os.path.exists(path):\n",
    "        return []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            return data if isinstance(data, list) else []\n",
    "        except json.JSONDecodeError:\n",
    "            return []\n",
    "\n",
    "def save_merged(path, items):\n",
    "    tmp = path + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(items, f, indent=2, ensure_ascii=False)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    subreddits = [\n",
    "        \"applyingtocollege\", \"csmajors\", \"btechtards\", \"recruitinghell\",\n",
    "        \"sysadmin\", \"careerguidance\", \"strangerthings\", \"unpopularopinion\",\n",
    "        \"ama\", \"casualconversation\", \"twohottakes\"\n",
    "    ]\n",
    "    listings = [\"controversial\", \"top\", \"hot\", \"new\"]\n",
    "\n",
    "    merged = load_merged(MERGED_PATH)\n",
    "    print(f\"Loaded {len(merged)} existing items from {MERGED_PATH}\")\n",
    "\n",
    "    new_items = []\n",
    "    for subreddit in subreddits:\n",
    "        group_id = str(uuid.uuid4())\n",
    "        for listing in listings:\n",
    "            try:\n",
    "                posts = fetch_reddit_posts(subreddit, listing, limit=50, time_filter=\"day\")\n",
    "                for p in posts:\n",
    "                    transformed = reddit_to_yikyak(p, listing, group_id)\n",
    "                    if transformed:\n",
    "                        new_items.append(transformed)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {listing} from {subreddit}: {e}\")\n",
    "\n",
    "    merged.extend(new_items)\n",
    "    save_merged(MERGED_PATH, merged)\n",
    "\n",
    "    with open(\"new_items_this_run.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(new_items, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Added {len(new_items)} items (no dedupe).\")\n",
    "    print(f\"Merged file now has {len(merged)} items: {MERGED_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (CSC371)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
